using System;
using System.Collections.Generic;
using System.Linq;
using System.Net;
using System.Net.Http;
using System.Threading;
using System.Threading.Tasks;
using Microsoft.Extensions.Logging;
using Lidarr.Plugin.Common.Utilities;

namespace Lidarr.Plugin.Common.Services.Network
{
    /// <summary>
    /// Network resilience service for coordinating batch operations and non-HTTP workflows.
    /// For outbound HTTP, this service delegates to the unified HTTP executor to prevent drift
    /// and avoid double-retry. Use <see cref="HttpClientExtensions.ExecuteWithResilienceAsync(HttpClient, HttpRequestMessage, ResiliencePolicy, CancellationToken)"/>
    /// for all HTTP calls; this service remains as a thin adapter for batches and cross-cutting policies.
    /// </summary>
    /// <remarks>
    /// Critical Issue: Network failures during batch operations cause:
    /// - Partial album downloads with inconsistent state
    /// - No recovery mechanism for failed operations
    /// - Lost progress on large operations
    /// - Corrupted cache states requiring manual cleanup
    /// 
    /// This service provides:
    /// 1. Atomic batch operations with all-or-nothing semantics
    /// 2. Automatic retry with exponential backoff for transient failures
    /// 3. Checkpoint-based recovery for large operations
    /// 4. Network health monitoring and circuit breaker pattern
    /// 5. Graceful degradation during extended outages
    /// </remarks>
    public class NetworkResilienceService
    {
        private readonly ILogger<NetworkResilienceService> _logger;
        private readonly NetworkHealthMonitor _healthMonitor;
        private readonly Dictionary<string, BatchOperationState> _activeBatchOperations;
        private readonly SemaphoreSlim _operationLock;

        // Circuit breaker settings
        private int _consecutiveFailures = 0;
        private DateTime _lastFailureTime = DateTime.MinValue;
        private readonly int _circuitBreakerThreshold = 5; // Failures before opening circuit
        private readonly TimeSpan _circuitBreakerTimeout = TimeSpan.FromMinutes(2); // How long to keep circuit open

        // Retry settings
        private readonly int _maxRetryAttempts = 3;
        private readonly TimeSpan _baseRetryDelay = TimeSpan.FromSeconds(1);
        private readonly TimeSpan _maxRetryDelay = TimeSpan.FromSeconds(30);

        // Checkpoint settings
        private readonly int _checkpointInterval = 10; // Save progress every 10 operations

        public NetworkResilienceService(ILogger<NetworkResilienceService> logger)
        {
            _logger = logger ?? throw new ArgumentNullException(nameof(logger));
            _healthMonitor = new NetworkHealthMonitor(_logger);
            _activeBatchOperations = new Dictionary<string, BatchOperationState>();
            _operationLock = new SemaphoreSlim(1, 1);
        }

        /// <summary>
        /// Thin adapter that executes a batch of HTTP requests via the shared HTTP resilience executor.
        /// Avoids double-retry by delegating all retry behavior to <see cref="HttpClientExtensions.ExecuteWithResilienceAsync(HttpClient, HttpRequestMessage, ResiliencePolicy, CancellationToken)"/>.
        /// Batch-level checkpoints/circuit-breaker are still honored.
        /// </summary>
        public async Task<ResilientBatchResult<HttpResponseMessage>> ExecuteHttpBatchAsync(
            string operationId,
            IEnumerable<HttpRequestMessage> requests,
            HttpClient httpClient,
            ResiliencePolicy policy,
            NetworkResilienceOptions options = null,
            IProgress<BatchProgress> progress = null,
            CancellationToken cancellationToken = default)
        {
            if (httpClient == null) throw new ArgumentNullException(nameof(httpClient));
            if (policy == null) throw new ArgumentNullException(nameof(policy));
            options ??= NetworkResilienceOptions.Default;

            var requestList = requests?.ToList() ?? new List<HttpRequestMessage>();
            var total = requestList.Count;
            var result = new ResilientBatchResult<HttpResponseMessage>
            {
                OperationId = operationId,
                IsSuccessful = true
            };

            var started = DateTime.UtcNow;
            for (int i = 0; i < total; i++)
            {
                cancellationToken.ThrowIfCancellationRequested();

                if (IsCircuitBreakerOpen())
                {
                    _logger.LogWarning("Circuit breaker open; aborting HTTP batch '{OperationId}'", operationId);
                    result.IsSuccessful = false;
                    result.FailureReason = "Circuit breaker open";
                    result.CanRetryLater = true;
                    break;
                }

                var req = requestList[i];
                try
                {
                    // Delegate resilience to central HTTP path; no local retry here
                    var resp = await httpClient.ExecuteWithResilienceAsync(req, policy, cancellationToken).ConfigureAwait(false);
                    RecordSuccess();
                    result.Results.Add(resp);
                }
                catch (Exception ex)
                {
                    RecordFailure();
                    result.IsSuccessful = false;
                    result.Failures.Add(new BatchItemFailure
                    {
                        ItemIndex = i,
                        Error = ex,
                        CanRetry = false
                    });

                    if (!options.ContinueOnFailure)
                    {
                        result.FailureReason = ex.Message;
                        break;
                    }
                }

                progress?.Report(new BatchProgress
                {
                    CompletedItems = i + 1,
                    TotalItems = total,
                    SuccessfulItems = result.Results.Count,
                    FailedItems = result.Failures.Count,
                    CurrentItem = i
                });
            }

            result.CompletionTime = DateTime.UtcNow - started;
            return result;
        }

        /// <summary>
        /// Executes a batch operation with full resilience including checkpoints and rollback
        /// </summary>
        /// <typeparam name="TInput">Type of input items to process</typeparam>
        /// <typeparam name="TResult">Type of results from processing</typeparam>
        /// <param name="operationId">Unique identifier for the operation</param>
        /// <param name="items">Items to process in the batch</param>
        /// <param name="processor">Function to process each item</param>
        /// <param name="options">Resilience options for the operation</param>
        /// <param name="progress">Progress reporter</param>
        /// <param name="cancellationToken">Cancellation token</param>
        /// <returns>Results from the batch operation</returns>
        public async Task<ResilientBatchResult<TResult>> ExecuteResilientBatchAsync<TInput, TResult>(
            string operationId,
            IEnumerable<TInput> items,
            Func<TInput, CancellationToken, Task<TResult>> processor,
            NetworkResilienceOptions options = null,
            IProgress<BatchProgress> progress = null,
            CancellationToken cancellationToken = default)
        {
            options ??= NetworkResilienceOptions.Default;
            var itemList = items.ToList();

            if (!itemList.Any())
            {
                return new ResilientBatchResult<TResult>
                {
                    IsSuccessful = true,
                    Results = new List<TResult>(),
                    OperationId = operationId
                };
            }

            _logger.LogInformation("üõ°Ô∏è RESILIENT BATCH START: Operation '{0}' with {1} items", operationId, itemList.Count);

            await _operationLock.WaitAsync(cancellationToken);
            try
            {
                // Check if operation can be resumed from checkpoint
                var existingOperation = await LoadExistingOperationAsync(operationId);
                var batchState = existingOperation ?? new BatchOperationState
                {
                    OperationId = operationId,
                    TotalItems = itemList.Count,
                    StartTime = DateTime.UtcNow,
                    Options = options
                };

                _activeBatchOperations[operationId] = batchState;

                // Execute the batch with resilience
                var result = await ExecuteBatchWithRecoveryAsync(
                    batchState, itemList, processor, progress, cancellationToken);

                // Cleanup successful operation
                await CleanupOperationAsync(operationId);

                return result;
            }
            finally
            {
                _operationLock.Release();
            }
        }

        /// <summary>
        /// Core batch execution with recovery and checkpoint logic
        /// </summary>
        private async Task<ResilientBatchResult<TResult>> ExecuteBatchWithRecoveryAsync<TInput, TResult>(
            BatchOperationState batchState,
            List<TInput> items,
            Func<TInput, CancellationToken, Task<TResult>> processor,
            IProgress<BatchProgress> progress,
            CancellationToken cancellationToken)
        {
            var results = new List<TResult>(batchState.Results?.Cast<TResult>() ?? new List<TResult>());
            var failures = new List<BatchItemFailure>();
            var startIndex = batchState.CompletedItems;

            for (int i = startIndex; i < items.Count; i++)
            {
                cancellationToken.ThrowIfCancellationRequested();

                // Check circuit breaker before each operation
                if (IsCircuitBreakerOpen())
                {
                    _logger.LogWarning("‚ö° CIRCUIT BREAKER OPEN: Failing fast for operation '{0}' at item {1}",
                                batchState.OperationId, i);

                    return new ResilientBatchResult<TResult>
                    {
                        IsSuccessful = false,
                        Results = results,
                        OperationId = batchState.OperationId,
                        FailureReason = "Circuit breaker is open due to repeated network failures",
                        CanRetryLater = true
                    };
                }

                var item = items[i];
                var itemResult = await ProcessItemWithRetryAsync(item, processor, batchState.Options, cancellationToken);

                if (itemResult.IsSuccess)
                {
                    results.Add(itemResult.Result);
                    batchState.CompletedItems++;
                    RecordSuccess();
                }
                else
                {
                    failures.Add(new BatchItemFailure
                    {
                        ItemIndex = i,
                        Error = itemResult.Exception,
                        CanRetry = itemResult.CanRetry
                    });

                    RecordFailure();

                    // Handle failure based on options
                    if (!batchState.Options.ContinueOnFailure)
                    {
                        _logger.LogError("üö´ BATCH FAILED: Stopping at first failure for operation '{0}' at item {1}",
                                     batchState.OperationId, i);

                        // Save checkpoint before failing
                        batchState.Results = results.Cast<object>().ToList();
                        batchState.Failures = failures;
                        await SaveCheckpointAsync(batchState);

                        return new ResilientBatchResult<TResult>
                        {
                            IsSuccessful = false,
                            Results = results,
                            OperationId = batchState.OperationId,
                            Failures = failures,
                            FailureReason = itemResult.Exception?.Message ?? "Item processing failed",
                            CanRetryFromCheckpoint = true
                        };
                    }
                }

                // Save checkpoint periodically
                if ((i + 1) % _checkpointInterval == 0)
                {
                    batchState.Results = results.Cast<object>().ToList();
                    batchState.Failures = failures;
                    await SaveCheckpointAsync(batchState);

                    _logger.LogDebug("üíæ CHECKPOINT: Saved progress for operation '{0}' at item {1}/{2}",
                                 batchState.OperationId, i + 1, items.Count);
                }

                // Report progress
                progress?.Report(new BatchProgress
                {
                    CompletedItems = batchState.CompletedItems,
                    TotalItems = batchState.TotalItems,
                    SuccessfulItems = results.Count,
                    FailedItems = failures.Count,
                    CurrentItem = i + 1
                });
            }

            // Final result
            var isSuccessful = failures.Count == 0 ||
                              (batchState.Options.ContinueOnFailure && results.Any());

            _logger.LogInformation("üõ°Ô∏è RESILIENT BATCH COMPLETE: Operation '{0}' - {1}/{2} successful",
                        batchState.OperationId, results.Count, items.Count);

            return new ResilientBatchResult<TResult>
            {
                IsSuccessful = isSuccessful,
                Results = results,
                OperationId = batchState.OperationId,
                Failures = failures,
                CompletionTime = DateTime.UtcNow - batchState.StartTime
            };
        }

        /// <summary>
        /// Processes a single item with retry logic and failure classification
        /// </summary>
        private async Task<ItemProcessingResult<TResult>> ProcessItemWithRetryAsync<TInput, TResult>(
            TInput item,
            Func<TInput, CancellationToken, Task<TResult>> processor,
            NetworkResilienceOptions options,
            CancellationToken cancellationToken)
        {
            Exception? lastException = null;

            for (int attempt = 0; attempt < _maxRetryAttempts; attempt++)
            {
                try
                {
                    var result = await processor(item, cancellationToken);

                    if (attempt > 0)
                    {
                        _logger.LogDebug("‚úÖ RETRY SUCCESS: Item processed successfully on attempt {0}", attempt + 1);
                    }

                    return new ItemProcessingResult<TResult>
                    {
                        IsSuccess = true,
                        Result = result
                    };
                }
                catch (OperationCanceledException)
                {
                    throw; // Don't retry cancellation
                }
                catch (Exception ex)
                {
                    lastException = ex;

                    if (!ShouldRetry(ex, attempt))
                    {
                        break; // Don't retry this type of exception
                    }

                    if (attempt < _maxRetryAttempts - 1)
                    {
                        var delay = CalculateRetryDelay(attempt);
                        _logger.LogDebug("üîÑ RETRY: Attempt {0} failed, retrying in {1}ms: {2}",
                                     attempt + 1, delay.TotalMilliseconds, ex.Message);

                        await Task.Delay(delay, cancellationToken);
                    }
                }
            }

            var canRetry = ShouldRetry(lastException, 0);

            return new ItemProcessingResult<TResult>
            {
                IsSuccess = false,
                Exception = lastException,
                CanRetry = canRetry
            };
        }

        /// <summary>
        /// Determines if an exception should trigger a retry
        /// </summary>
        private bool ShouldRetry(Exception exception, int attemptNumber)
        {
            if (attemptNumber >= _maxRetryAttempts - 1)
                return false;

            // Retry network-related exceptions
            return exception is TimeoutException ||
                   exception is HttpRequestException ||
                   exception is WebException ||
                   (exception is TaskCanceledException && !exception.Message.Contains("A task was canceled"));
        }

        /// <summary>
        /// Calculates exponential backoff delay for retries
        /// </summary>
        private TimeSpan CalculateRetryDelay(int attemptNumber)
        {
            var delay = TimeSpan.FromMilliseconds(_baseRetryDelay.TotalMilliseconds * Math.Pow(2, attemptNumber));
            return delay > _maxRetryDelay ? _maxRetryDelay : delay;
        }

        /// <summary>
        /// Circuit breaker logic to fail fast during extended outages
        /// </summary>
        private bool IsCircuitBreakerOpen()
        {
            if (_consecutiveFailures < _circuitBreakerThreshold)
                return false;

            if (DateTime.UtcNow - _lastFailureTime > _circuitBreakerTimeout)
            {
                _logger.LogInformation("üîÑ CIRCUIT BREAKER: Attempting to close circuit after timeout");
                _consecutiveFailures = 0; // Reset circuit breaker
                return false;
            }

            return true;
        }

        /// <summary>
        /// Records successful operation for circuit breaker
        /// </summary>
        private void RecordSuccess()
        {
            if (_consecutiveFailures > 0)
            {
                _logger.LogInformation("‚úÖ CIRCUIT BREAKER: Resetting failure count after successful operation");
                _consecutiveFailures = 0;
            }
        }

        /// <summary>
        /// Records failed operation for circuit breaker
        /// </summary>
        private void RecordFailure()
        {
            _consecutiveFailures++;
            _lastFailureTime = DateTime.UtcNow;

            if (_consecutiveFailures == _circuitBreakerThreshold)
            {
                _logger.LogWarning("‚ö° CIRCUIT BREAKER: Opening circuit after {0} consecutive failures", _consecutiveFailures);
            }
        }

        #region Checkpoint Management

        /// <summary>
        /// Saves operation checkpoint for recovery
        /// </summary>
#pragma warning disable CS1998
        private async Task SaveCheckpointAsync(BatchOperationState batchState)
        {
            try
            {
                // In a real implementation, this would save to persistent storage
                // For now, we keep it in memory
                batchState.LastCheckpointTime = DateTime.UtcNow;

                _logger.LogDebug("üíæ CHECKPOINT SAVED: Operation '{0}' at {1}/{2} items",
                             batchState.OperationId, batchState.CompletedItems, batchState.TotalItems);
            }
            catch (Exception ex)
            {
                _logger.LogError(ex, "Failed to save checkpoint for operation: {0}", batchState.OperationId);
                // Don't fail the operation if checkpoint saving fails
            }
        }

        /// <summary>
        /// Loads existing operation from checkpoint
        /// </summary>
        private async Task<BatchOperationState> LoadExistingOperationAsync(string operationId)
        {
            try
            {
                // In a real implementation, this would load from persistent storage
                if (_activeBatchOperations.TryGetValue(operationId, out var existingState))
                {
                    _logger.LogInformation("üîÑ RESUMING: Found existing operation '{0}' at {1}/{2} items",
                                operationId, existingState.CompletedItems, existingState.TotalItems);
                    return existingState;
                }
            }
            catch (Exception ex)
            {
                _logger.LogError(ex, "Failed to load existing operation: {0}", operationId);
            }

            return null;
        }

        /// <summary>
        /// Cleans up completed operation
        /// </summary>
        private async Task CleanupOperationAsync(string operationId)
        {
            try
            {
                _activeBatchOperations.Remove(operationId);
                _logger.LogDebug("üßπ CLEANUP: Removed completed operation '{0}'", operationId);
            }
            catch (Exception ex)
            {
                _logger.LogError(ex, "Failed to cleanup operation: {0}", operationId);
            }
        }
#pragma warning restore CS1998

        #endregion

        /// <summary>
        /// Gets current network health status
        /// </summary>
        public NetworkHealthStatus GetNetworkHealth()
        {
            return _healthMonitor.GetCurrentHealth();
        }

        /// <summary>
        /// Gets statistics for monitoring
        /// </summary>
        public NetworkResilienceStatistics GetStatistics()
        {
            return new NetworkResilienceStatistics
            {
                ActiveOperations = _activeBatchOperations.Count,
                ConsecutiveFailures = _consecutiveFailures,
                CircuitBreakerOpen = IsCircuitBreakerOpen(),
                LastFailureTime = _lastFailureTime,
                NetworkHealth = _healthMonitor.GetCurrentHealth()
            };
        }
    }

    #region Supporting Classes

    /// <summary>
    /// Monitors network health and connectivity
    /// </summary>
    public class NetworkHealthMonitor
    {
        private readonly ILogger<NetworkResilienceService> _logger;
        private NetworkHealthStatus _currentHealth = NetworkHealthStatus.Healthy;
        private DateTime _lastHealthCheck = DateTime.MinValue;

        public NetworkHealthMonitor(ILogger<NetworkResilienceService> logger)
        {
            _logger = logger;
        }

        public NetworkHealthStatus GetCurrentHealth()
        {
            // In a real implementation, this would perform actual network checks
            return _currentHealth;
        }
    }

    /// <summary>
    /// State tracking for batch operations
    /// </summary>
    public class BatchOperationState
    {
        public string OperationId { get; set; }
        public int TotalItems { get; set; }
        public int CompletedItems { get; set; }
        public DateTime StartTime { get; set; }
        public DateTime LastCheckpointTime { get; set; }
        public List<object> Results { get; set; } = new();
        public List<BatchItemFailure> Failures { get; set; } = new();
        public NetworkResilienceOptions Options { get; set; }
    }

    /// <summary>
    /// Options for network resilience behavior
    /// </summary>
    public class NetworkResilienceOptions
    {
        public bool ContinueOnFailure { get; set; } = true;
        public bool EnableCheckpoints { get; set; } = true;
        public int CheckpointInterval { get; set; } = 10;
        public int MaxRetryAttempts { get; set; } = 3;
        public TimeSpan RetryDelay { get; set; } = TimeSpan.FromSeconds(1);

        public static NetworkResilienceOptions Default => new();

        public static NetworkResilienceOptions StrictMode => new()
        {
            ContinueOnFailure = false,
            EnableCheckpoints = true,
            MaxRetryAttempts = 5
        };
    }

    /// <summary>
    /// Result of processing a single item
    /// </summary>
    public class ItemProcessingResult<TResult>
    {
        public bool IsSuccess { get; set; }
        public TResult Result { get; set; }
        public Exception Exception { get; set; }
        public bool CanRetry { get; set; }
    }

    /// <summary>
    /// Information about a failed batch item
    /// </summary>
    public class BatchItemFailure
    {
        public int ItemIndex { get; set; }
        public Exception Error { get; set; }
        public bool CanRetry { get; set; }
        public DateTime FailureTime { get; set; } = DateTime.UtcNow;
    }

    /// <summary>
    /// Progress information for batch operations
    /// </summary>
    public class BatchProgress
    {
        public int CompletedItems { get; set; }
        public int TotalItems { get; set; }
        public int SuccessfulItems { get; set; }
        public int FailedItems { get; set; }
        public int CurrentItem { get; set; }

        public double PercentComplete => TotalItems > 0 ? (double)CompletedItems / TotalItems * 100 : 0;
    }

    /// <summary>
    /// Result of a resilient batch operation
    /// </summary>
    public class ResilientBatchResult<TResult>
    {
        public bool IsSuccessful { get; set; }
        public List<TResult> Results { get; set; } = new();
        public string OperationId { get; set; }
        public List<BatchItemFailure> Failures { get; set; } = new();
        public string FailureReason { get; set; }
        public bool CanRetryLater { get; set; }
        public bool CanRetryFromCheckpoint { get; set; }
        public TimeSpan CompletionTime { get; set; }

        public double SuccessRate => Results.Count + Failures.Count > 0 ?
            (double)Results.Count / (Results.Count + Failures.Count) : 0;
    }

    /// <summary>
    /// Network health status enumeration
    /// </summary>
    public enum NetworkHealthStatus
    {
        Healthy,
        Degraded,
        Unhealthy,
        Unknown
    }

    /// <summary>
    /// Statistics for monitoring network resilience
    /// </summary>
    public class NetworkResilienceStatistics
    {
        public int ActiveOperations { get; set; }
        public int ConsecutiveFailures { get; set; }
        public bool CircuitBreakerOpen { get; set; }
        public DateTime LastFailureTime { get; set; }
        public NetworkHealthStatus NetworkHealth { get; set; }
    }

    #endregion
}
